{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.6.9",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "colab": {
      "name": "converted.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tom1484/keras-estimator/blob/master/keras-estimator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uCwm9cwkagS7"
      },
      "source": [
        "# **Compare Estimator with Keras**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3u74a7gQ5Uoy"
      },
      "source": [
        "## **Basic definitions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmfA5dbPYVKB",
        "colab_type": "code",
        "outputId": "a973d0b5-8375-45a0-f931-585db87487bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow==2.0.0b1\n",
        "import tensorflow as tf\n",
        "from tensorflow import data\n",
        "from tensorflow import estimator\n",
        "from tensorflow.keras import *\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0b1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/6c/2c9a5c4d095c63c2fb37d20def0e4f92685f7aee9243d6aae25862694fd1/tensorflow-2.0.0b1-cp36-cp36m-manylinux1_x86_64.whl (87.9MB)\n",
            "\u001b[K     |████████████████████████████████| 87.9MB 35kB/s \n",
            "\u001b[?25hCollecting tb-nightly<1.14.0a20190604,>=1.14.0a20190603\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/96/571b875cd81dda9d5dfa1422a4f9d749e67c0a8d4f4f0b33a4e5f5f35e27/tb_nightly-1.14.0a20190603-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 29.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (0.33.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (0.1.8)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (1.17.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (1.11.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (3.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (1.1.0)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/dd/99c47dd007dcf10d63fd895611b063732646f23059c618a373e85019eb0e/tf_estimator_nightly-1.14.0.dev2019060501-py2.py3-none-any.whl (496kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 47.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (1.0.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0b1) (1.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0b1) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0b1) (0.16.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0b1) (42.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0b1) (2.8.0)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, tensorflow\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed tb-nightly-1.14.0a20190603 tensorflow-2.0.0b1 tf-estimator-nightly-1.14.0.dev2019060501\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXOlrBo_kquP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input_fn is an interface to feed data to the model\n",
        "def input_fn(x, y, batch_size=None, num_epochs=None):\n",
        "  dataset = data.Dataset.from_tensor_slices((x, y))\n",
        "  if batch_size:\n",
        "    dataset = dataset.batch(batch_size)\n",
        "  if num_epochs:\n",
        "    dataset = dataset.repeat(num_epochs)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MnIHv-Ao5dMv",
        "colab": {}
      },
      "source": [
        "input_shape = (28, 28, 1)\n",
        "num_train = 60000\n",
        "num_test = 10000\n",
        "\n",
        "batch_size = 100\n",
        "train_epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SPeBLjUPJA0G"
      },
      "source": [
        "## **Import dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vBXzw1F1ggo2",
        "outputId": "8729c8bb-b03c-4c4c-cdaf-a0c9c476338d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
        "\n",
        "# constraint the values between 0 and 1\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# each image has only one channel\n",
        "x_train.resize((len(x_train), ) + input_shape)\n",
        "x_test.resize((len(x_test), ) + input_shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-5sHLN2XfPCc"
      },
      "source": [
        "## **Visualization of dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eYm8Gr4pgWLD",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "0e5f3f77-dd46-4419-e57e-12b149ae287b",
        "colab_type": "code",
        "id": "McTSCzJrgzjX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# show images in a 4 * 4 tables\n",
        "n = 4 * 4\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(x_train[i][:,:,0])\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAScAAAEhCAYAAAA9A2ZcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 360x360 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PhAHoNV9i6pE"
      },
      "source": [
        "## **Model structure**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1xh6uEkMkW79"
      },
      "source": [
        "| Layer | Output Shape |\n",
        "|:-|:-|\n",
        "| conv2d | (None, 26, 26, 64) |\n",
        "| max_pooling2d | (None, 13, 13, 64) |\n",
        "| conv2d | (None, 11, 11, 32) |\n",
        "| max_pooling2d | (None,  6, 6,  32) |\n",
        "| conv2d | (None,  4, 4, 16) |\n",
        "| flattem | (None, 256) |       \n",
        "| dense | (None, 10) |   \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6WTuOiMRPf8C"
      },
      "source": [
        "## **Estimator**\n",
        "\n",
        "The Estimator is a high-level API of Tensorflow, which was early added to Tensorflow in Release 1.1. It provides a abstraction over low-level operations, which enormously reduces the codes we need to write.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://www.tensorflow.org/images/tensorflow_programming_environment.png\" width=\"70%\"></img></p>\n",
        "\n",
        "Estimator provides some premade models such as DNNClassifier and LinearClassifier. You can also make your own model to be trained by write a custom **model_fn**. model_fn takes 4 parameters: **features**, **labels**, **mode** and **params**. The funtions of the 4 parameters will be explained later.\n",
        "\n",
        "Below is an simple **image classfier** built with Estimator:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPuPHw_4vQqJ",
        "colab_type": "code",
        "outputId": "27908530-e3e7-4e02-e201-f53c6f3282d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "# this is the format of the definition of model_fn\n",
        "def model_fn(features, labels, mode, params):\n",
        "\n",
        "  # build model with keras.layers\n",
        "  inputs = layers.Input(shape=input_shape)\n",
        "  net = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
        "  net = layers.MaxPooling2D()(net)\n",
        "  net = layers.Conv2D(16, 3, activation=\"relu\")(net)\n",
        "  net = layers.MaxPooling2D()(net)\n",
        "  net = layers.Conv2D(8, 3, activation=\"relu\")(net)\n",
        "  net = layers.Flatten()(net)\n",
        "  outputs = layers.Dense(10, activation=\"softmax\")(net)\n",
        "\n",
        "  # this part creates a way to feed data\n",
        "  # I didn't notice that first time, so I used a tensor without any input\n",
        "  model = models.Model(inputs, outputs)\n",
        "  logits = model(features)\n",
        "\n",
        "  y_pred = tf.argmax(logits, axis=1)\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    return tf.estimator.EstimatorSpec(mode=mode, predictions=y_pred)\n",
        "  else:\n",
        "    cross_entropy = losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "    loss = tf.reduce_mean(input_tensor=cross_entropy)\n",
        "\n",
        "    optimizer = tf.compat.v1.train.AdamOptimizer()\n",
        "    train_op = optimizer.minimize(loss, global_step=tf.compat.v1.train.get_global_step())\n",
        "\n",
        "    metrics_dict = {\"accuracy\": tf.compat.v1.metrics.accuracy(labels, y_pred)}\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "      spec = tf.estimator.EstimatorSpec(mode=mode,\n",
        "                                        loss=loss,\n",
        "                                        train_op=train_op,\n",
        "                                        eval_metric_ops=metrics_dict)\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "      spec = tf.estimator.EstimatorSpec(mode=mode,\n",
        "                                        loss=loss,\n",
        "                                        eval_metric_ops=metrics_dict)\n",
        "\n",
        "    return spec\n",
        "\n",
        "estimator_classfier = tf.estimator.Estimator(model_fn=model_fn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpyt_di75h\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpyt_di75h', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f995c75e128>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "WARNING:tensorflow:Estimator's model_fn (<function model_fn at 0x7f995baa5598>) includes params argument, but params are not passed to Estimator.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "42ecaba1-cadc-4a46-ad33-1bfaf52b53bc",
        "colab_type": "code",
        "id": "jenTE9iK6mU0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_input_fn = lambda: input_fn(x_train, y_train, batch_size=batch_size, num_epochs=train_epochs)\n",
        "estimator_classfier.train(input_fn=train_input_fn, \n",
        "                          max_steps=num_train / batch_size * train_epochs)\n",
        "\n",
        "eval_input_fn = lambda: input_fn(x_train, y_train, batch_size=batch_size, num_epochs=1)\n",
        "estimator_classfier.evaluate(input_fn=eval_input_fn, \n",
        "                             steps=num_train / batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:1340: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpyt_di75h/model.ckpt.\n",
            "INFO:tensorflow:loss = 2.3033042, step = 0\n",
            "INFO:tensorflow:global_step/sec: 26.6143\n",
            "INFO:tensorflow:loss = 1.808445, step = 100 (3.759 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.6164\n",
            "INFO:tensorflow:loss = 1.7463914, step = 200 (3.621 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.6935\n",
            "INFO:tensorflow:loss = 1.6503816, step = 300 (3.611 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.5665\n",
            "INFO:tensorflow:loss = 1.6263707, step = 400 (3.627 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.7596\n",
            "INFO:tensorflow:loss = 1.6355118, step = 500 (3.603 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.7402\n",
            "INFO:tensorflow:loss = 1.612164, step = 600 (3.605 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.3521\n",
            "INFO:tensorflow:loss = 1.6434331, step = 700 (3.656 sec)\n",
            "INFO:tensorflow:global_step/sec: 26.3699\n",
            "INFO:tensorflow:loss = 1.6425328, step = 800 (3.792 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.3902\n",
            "INFO:tensorflow:loss = 1.596884, step = 900 (3.651 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.5677\n",
            "INFO:tensorflow:loss = 1.596452, step = 1000 (3.627 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.4\n",
            "INFO:tensorflow:loss = 1.5976956, step = 1100 (3.651 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.5208\n",
            "INFO:tensorflow:loss = 1.5894308, step = 1200 (3.632 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.4691\n",
            "INFO:tensorflow:loss = 1.6084487, step = 1300 (3.641 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.5781\n",
            "INFO:tensorflow:loss = 1.6158572, step = 1400 (3.627 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.5802\n",
            "INFO:tensorflow:loss = 1.58146, step = 1500 (3.625 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.6381\n",
            "INFO:tensorflow:loss = 1.5863091, step = 1600 (3.618 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.313\n",
            "INFO:tensorflow:loss = 1.5868974, step = 1700 (3.661 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.692\n",
            "INFO:tensorflow:loss = 1.5745234, step = 1800 (3.611 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.2255\n",
            "INFO:tensorflow:loss = 1.604726, step = 1900 (3.673 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.6833\n",
            "INFO:tensorflow:loss = 1.6011659, step = 2000 (3.615 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.2444\n",
            "INFO:tensorflow:loss = 1.5742673, step = 2100 (3.668 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.3799\n",
            "INFO:tensorflow:loss = 1.5813544, step = 2200 (3.651 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.5941\n",
            "INFO:tensorflow:loss = 1.585787, step = 2300 (3.624 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.7751\n",
            "INFO:tensorflow:loss = 1.577412, step = 2400 (3.600 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.5643\n",
            "INFO:tensorflow:loss = 1.5960486, step = 2500 (3.629 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.3255\n",
            "INFO:tensorflow:loss = 1.6012237, step = 2600 (3.659 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.5883\n",
            "INFO:tensorflow:loss = 1.5674107, step = 2700 (3.625 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.3696\n",
            "INFO:tensorflow:loss = 1.5811733, step = 2800 (3.654 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.3738\n",
            "INFO:tensorflow:loss = 1.5793806, step = 2900 (3.653 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.5647\n",
            "INFO:tensorflow:loss = 1.5626779, step = 3000 (3.628 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.559\n",
            "INFO:tensorflow:loss = 1.575571, step = 3100 (3.628 sec)\n",
            "INFO:tensorflow:global_step/sec: 28.1364\n",
            "INFO:tensorflow:loss = 1.6015637, step = 3200 (3.554 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.5805\n",
            "INFO:tensorflow:loss = 1.5710521, step = 3300 (3.627 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.9634\n",
            "INFO:tensorflow:loss = 1.5750859, step = 3400 (3.575 sec)\n",
            "INFO:tensorflow:global_step/sec: 28.2157\n",
            "INFO:tensorflow:loss = 1.5812961, step = 3500 (3.544 sec)\n",
            "INFO:tensorflow:global_step/sec: 28.0586\n",
            "INFO:tensorflow:loss = 1.5648822, step = 3600 (3.564 sec)\n",
            "INFO:tensorflow:global_step/sec: 28.1509\n",
            "INFO:tensorflow:loss = 1.570944, step = 3700 (3.552 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.8923\n",
            "INFO:tensorflow:loss = 1.601809, step = 3800 (3.586 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.7992\n",
            "INFO:tensorflow:loss = 1.5654544, step = 3900 (3.597 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.9384\n",
            "INFO:tensorflow:loss = 1.5778961, step = 4000 (3.580 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.7879\n",
            "INFO:tensorflow:loss = 1.5808234, step = 4100 (3.600 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.7541\n",
            "INFO:tensorflow:loss = 1.5627255, step = 4200 (3.603 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.6219\n",
            "INFO:tensorflow:loss = 1.570834, step = 4300 (3.620 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.8439\n",
            "INFO:tensorflow:loss = 1.6008692, step = 4400 (3.591 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.5903\n",
            "INFO:tensorflow:loss = 1.5659659, step = 4500 (3.624 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.8742\n",
            "INFO:tensorflow:loss = 1.5708965, step = 4600 (3.588 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.8278\n",
            "INFO:tensorflow:loss = 1.5794766, step = 4700 (3.594 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.7147\n",
            "INFO:tensorflow:loss = 1.5529763, step = 4800 (3.608 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.9731\n",
            "INFO:tensorflow:loss = 1.5688134, step = 4900 (3.575 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.8621\n",
            "INFO:tensorflow:loss = 1.5990597, step = 5000 (3.589 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.8028\n",
            "INFO:tensorflow:loss = 1.564886, step = 5100 (3.597 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.6652\n",
            "INFO:tensorflow:loss = 1.5718603, step = 5200 (3.615 sec)\n",
            "INFO:tensorflow:global_step/sec: 26.9851\n",
            "INFO:tensorflow:loss = 1.5830504, step = 5300 (3.706 sec)\n",
            "INFO:tensorflow:global_step/sec: 26.8282\n",
            "INFO:tensorflow:loss = 1.5563334, step = 5400 (3.729 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.1017\n",
            "INFO:tensorflow:loss = 1.5655849, step = 5500 (3.688 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.6383\n",
            "INFO:tensorflow:loss = 1.5929422, step = 5600 (3.621 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.7613\n",
            "INFO:tensorflow:loss = 1.565105, step = 5700 (3.599 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.9937\n",
            "INFO:tensorflow:loss = 1.5744977, step = 5800 (3.572 sec)\n",
            "INFO:tensorflow:global_step/sec: 27.684\n",
            "INFO:tensorflow:loss = 1.59147, step = 5900 (3.612 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 6000 into /tmp/tmpyt_di75h/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 1.5920262.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-12-28T17:56:04Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmpyt_di75h/model.ckpt-6000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [60/600]\n",
            "INFO:tensorflow:Evaluation [120/600]\n",
            "INFO:tensorflow:Evaluation [180/600]\n",
            "INFO:tensorflow:Evaluation [240/600]\n",
            "INFO:tensorflow:Evaluation [300/600]\n",
            "INFO:tensorflow:Evaluation [360/600]\n",
            "INFO:tensorflow:Evaluation [420/600]\n",
            "INFO:tensorflow:Evaluation [480/600]\n",
            "INFO:tensorflow:Evaluation [540/600]\n",
            "INFO:tensorflow:Evaluation [600/600]\n",
            "INFO:tensorflow:Finished evaluation at 2019-12-28-17:56:12\n",
            "INFO:tensorflow:Saving dict for global step 6000: accuracy = 0.88816667, global_step = 6000, loss = 1.5725749\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6000: /tmp/tmpyt_di75h/model.ckpt-6000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.88816667, 'global_step': 6000, 'loss': 1.5725749}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "07064796-d1b8-4016-c120-e22e59054947",
        "colab_type": "code",
        "id": "LrwFbdGRKFn_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "pred_input_fn = lambda: input_fn(x_test, y_test, batch_size=batch_size, num_epochs=1)\n",
        "predict = estimator_classfier.predict(pred_input_fn)\n",
        "\n",
        "# convert a predict object to an array\n",
        "predict = [pred for pred in predict]\n",
        "\n",
        "errors = 0\n",
        "for i in range(num_test):\n",
        "  if predict[i] != y_test[i]:\n",
        "    errors += 1\n",
        "print(f\"final accuracy: {(1 - errors / num_test) * 100 :.2f}%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmpyt_di75h/model.ckpt-6000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "final accuracy: 88.79%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MyG2Gi1gamHV"
      },
      "source": [
        "## **Keras**\n",
        "\n",
        "Keras is a model-level API, and doesn't handle low-level \n",
        "operations, which is the job of tensor manipulation libraries, or backends. Keras supports three backends - Tensorflow, Theano and CNTK.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://3.bp.blogspot.com/-l2UT45WGdyw/Wbe7au1nfwI/AAAAAAAAD1I/GeQcQUUWezIiaFFRCiMILlX2EYdG49C0wCLcBGAs/s1600/image6.png\" width=\"70%\"></img></p>\n",
        "\n",
        "As a deep learning library, Keras provides many deep learning model components such as layers, activation functions and optimizers, which similar to Estimator.\n",
        "In fact, Keras wasn't added to Tensorflow until Release 1.4.0 (2017). Now, when you use tf.keras, you are simply using the Keras interface with the Tensorflow backend to build and train your model.\n",
        "\n",
        "Below is an **autoencoder** built with Keras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "ee3c1f09-ecd1-4365-fc12-5a8addfcd641",
        "colab_type": "code",
        "id": "SsVCIjz63RuN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "keras_classfier = models.Sequential([\n",
        "  layers.Input(shape=input_shape),\n",
        "  layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "  layers.MaxPooling2D((2, 2)),\n",
        "  layers.Conv2D(16, (3, 3), activation='relu'),\n",
        "  layers.MaxPooling2D((2, 2)),\n",
        "  layers.Conv2D(8, (3, 3), activation='relu'),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "keras_classfier.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "keras_classfier.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 16)        4624      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 3, 3, 8)           1160      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 72)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                730       \n",
            "=================================================================\n",
            "Total params: 6,834\n",
            "Trainable params: 6,834\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "f14e7deb-7c5d-42d5-9b24-87fc6dccbae5",
        "colab_type": "code",
        "id": "eU6PObOkw1jU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "train_input_fn = input_fn(x_train, y_train, batch_size=batch_size, num_epochs=train_epochs)\n",
        "keras_classfier.fit(train_input_fn, \n",
        "                    steps_per_epoch=num_train / batch_size, \n",
        "                    epochs=train_epochs)\n",
        "\n",
        "eval_input_fn = input_fn(x_train, y_train, batch_size=batch_size, num_epochs=1)\n",
        "keras_classfier.evaluate(eval_input_fn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.5027 - accuracy: 0.8445\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.1563 - accuracy: 0.9531\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 21s 34ms/step - loss: 0.1097 - accuracy: 0.9671\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0893 - accuracy: 0.9733\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0770 - accuracy: 0.9769\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0679 - accuracy: 0.9795\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0614 - accuracy: 0.9812\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0566 - accuracy: 0.9827\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0529 - accuracy: 0.9840\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 21s 35ms/step - loss: 0.0499 - accuracy: 0.9849\n",
            "600/600 [==============================] - 7s 12ms/step - loss: 0.0470 - accuracy: 0.9852\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.047005620267736956, 0.98525]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "6772ac32-02d3-44b3-fd36-f0ed9a15039d",
        "colab_type": "code",
        "id": "p5RY1tLqkcU3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "predict = keras_classfier.predict(x_test)\n",
        "# each prediction is a 10d vector\n",
        "predict = np.argmax(predict, axis=1)\n",
        "\n",
        "errors = 0\n",
        "for i in range(num_test):\n",
        "  if predict[i] != y_test[i]:\n",
        "    errors += 1\n",
        "print(f\"final accuracy: {(1 - errors / num_test) * 100 :.2f}%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final accuracy: 98.15%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "clncV8ZParyU"
      },
      "source": [
        "## **Important differences**\n",
        "\n",
        "Usually people use estimator because of a important difference that you can conduct distributed training across multiple servers with the Estimators API, but not with Keras API.\n",
        "\n",
        "According to **Tensorflow Estimators Guide**, it says that:\n",
        "\n",
        "> You can run Estimator-based models on a local host or on a **distributed multi-server** environment without changing your model. Furthermore, you can run Estimator-based models on CPUs, GPUs, or TPUs without recoding your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0jCw3MvXt89L"
      },
      "source": [
        "## **Convert Keras to Estimator**\n",
        "\n",
        "Both Estimator and Keras provides various models to work with and is easy to use. If you want to conduct distributed training across multiple servers, using Estimator is a better choice, but you can still work well with Keras because it provides the method: \n",
        "\n",
        "```\n",
        "tf.keras.estimator.model_to_estimator\n",
        "```\n",
        "\n",
        "Below is an example of how to **convert a Keras model to an Estimator model**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "1db62c60-5bf7-47fd-b8e5-fe90faf815c0",
        "colab_type": "code",
        "id": "kcTIeQWJDuuw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "from_keras = tf.keras.estimator.model_to_estimator(keras_classfier)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_uovurve\n",
            "INFO:tensorflow:Using the Keras model provided.\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_uovurve', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f995bf343c8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "00675f62-8ee4-4f76-871e-4fe8f9188e47",
        "colab_type": "code",
        "id": "VjrHCLgms2zj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_input_fn = lambda: input_fn(x_train, y_train, batch_size=batch_size, num_epochs=train_epochs)\n",
        "from_keras.train(input_fn=train_input_fn, \n",
        "                 max_steps=num_train / batch_size * train_epochs)\n",
        "\n",
        "eval_input_fn = lambda: input_fn(x_train, y_train, batch_size=batch_size, num_epochs=1)\n",
        "from_keras.evaluate(input_fn=eval_input_fn, \n",
        "                    steps=num_train / batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/tmp/tmp_uovurve/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
            "INFO:tensorflow:Warm-starting from: /tmp/tmp_uovurve/keras/keras_model.ckpt\n",
            "INFO:tensorflow:Warm-starting variables only in TRAINABLE_VARIABLES.\n",
            "INFO:tensorflow:Warm-started 8 variables.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp_uovurve/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.05091175, step = 0\n",
            "INFO:tensorflow:global_step/sec: 30.0125\n",
            "INFO:tensorflow:loss = 0.07514346, step = 100 (3.335 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.4231\n",
            "INFO:tensorflow:loss = 0.07333874, step = 200 (3.285 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.3382\n",
            "INFO:tensorflow:loss = 0.06442299, step = 300 (3.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.5056\n",
            "INFO:tensorflow:loss = 0.010086429, step = 400 (3.276 sec)\n",
            "INFO:tensorflow:global_step/sec: 29.7917\n",
            "INFO:tensorflow:loss = 0.013679669, step = 500 (3.358 sec)\n",
            "INFO:tensorflow:global_step/sec: 29.7781\n",
            "INFO:tensorflow:loss = 0.04279898, step = 600 (3.357 sec)\n",
            "INFO:tensorflow:global_step/sec: 29.8743\n",
            "INFO:tensorflow:loss = 0.075163975, step = 700 (3.347 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.507\n",
            "INFO:tensorflow:loss = 0.067489326, step = 800 (3.278 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.4688\n",
            "INFO:tensorflow:loss = 0.057642885, step = 900 (3.282 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.6258\n",
            "INFO:tensorflow:loss = 0.007817016, step = 1000 (3.265 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.5599\n",
            "INFO:tensorflow:loss = 0.0118693225, step = 1100 (3.272 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.432\n",
            "INFO:tensorflow:loss = 0.0446665, step = 1200 (3.286 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.627\n",
            "INFO:tensorflow:loss = 0.07463788, step = 1300 (3.265 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.6103\n",
            "INFO:tensorflow:loss = 0.06815862, step = 1400 (3.267 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.2204\n",
            "INFO:tensorflow:loss = 0.056599755, step = 1500 (3.309 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.6465\n",
            "INFO:tensorflow:loss = 0.0076526683, step = 1600 (3.263 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.6469\n",
            "INFO:tensorflow:loss = 0.011371897, step = 1700 (3.263 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.4671\n",
            "INFO:tensorflow:loss = 0.045167107, step = 1800 (3.283 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.3499\n",
            "INFO:tensorflow:loss = 0.07205404, step = 1900 (3.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.6274\n",
            "INFO:tensorflow:loss = 0.062254604, step = 2000 (3.265 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.4305\n",
            "INFO:tensorflow:loss = 0.055557717, step = 2100 (3.286 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.5669\n",
            "INFO:tensorflow:loss = 0.007244793, step = 2200 (3.271 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.677\n",
            "INFO:tensorflow:loss = 0.010357523, step = 2300 (3.260 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.2848\n",
            "INFO:tensorflow:loss = 0.046071753, step = 2400 (3.302 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.278\n",
            "INFO:tensorflow:loss = 0.07286973, step = 2500 (3.305 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.4314\n",
            "INFO:tensorflow:loss = 0.05725826, step = 2600 (3.286 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.4849\n",
            "INFO:tensorflow:loss = 0.055715438, step = 2700 (3.279 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.6649\n",
            "INFO:tensorflow:loss = 0.0063943495, step = 2800 (3.261 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.6319\n",
            "INFO:tensorflow:loss = 0.010447106, step = 2900 (3.265 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.6022\n",
            "INFO:tensorflow:loss = 0.04384704, step = 3000 (3.267 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.4939\n",
            "INFO:tensorflow:loss = 0.073483676, step = 3100 (3.280 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.6781\n",
            "INFO:tensorflow:loss = 0.046746336, step = 3200 (3.260 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.6159\n",
            "INFO:tensorflow:loss = 0.054489568, step = 3300 (3.267 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.2506\n",
            "INFO:tensorflow:loss = 0.0060024858, step = 3400 (3.305 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.7699\n",
            "INFO:tensorflow:loss = 0.010642114, step = 3500 (3.250 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.3354\n",
            "INFO:tensorflow:loss = 0.041553304, step = 3600 (3.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.4821\n",
            "INFO:tensorflow:loss = 0.07355647, step = 3700 (3.281 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.6335\n",
            "INFO:tensorflow:loss = 0.040481936, step = 3800 (3.265 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.5293\n",
            "INFO:tensorflow:loss = 0.055428136, step = 3900 (3.276 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.3176\n",
            "INFO:tensorflow:loss = 0.006040199, step = 4000 (3.300 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.605\n",
            "INFO:tensorflow:loss = 0.010873339, step = 4100 (3.266 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.6986\n",
            "INFO:tensorflow:loss = 0.042926162, step = 4200 (3.258 sec)\n",
            "INFO:tensorflow:global_step/sec: 29.7654\n",
            "INFO:tensorflow:loss = 0.07198093, step = 4300 (3.360 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.5439\n",
            "INFO:tensorflow:loss = 0.034850553, step = 4400 (3.274 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.781\n",
            "INFO:tensorflow:loss = 0.05639784, step = 4500 (3.251 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.346\n",
            "INFO:tensorflow:loss = 0.004856485, step = 4600 (3.293 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.0423\n",
            "INFO:tensorflow:loss = 0.0099786455, step = 4700 (3.329 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.5844\n",
            "INFO:tensorflow:loss = 0.041246265, step = 4800 (3.270 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.4859\n",
            "INFO:tensorflow:loss = 0.0728034, step = 4900 (3.280 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.7638\n",
            "INFO:tensorflow:loss = 0.028926231, step = 5000 (3.251 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.5294\n",
            "INFO:tensorflow:loss = 0.059915204, step = 5100 (3.275 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.7337\n",
            "INFO:tensorflow:loss = 0.005001236, step = 5200 (3.254 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.2903\n",
            "INFO:tensorflow:loss = 0.009589042, step = 5300 (3.301 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.4342\n",
            "INFO:tensorflow:loss = 0.041396517, step = 5400 (3.286 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.6974\n",
            "INFO:tensorflow:loss = 0.0747678, step = 5500 (3.258 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.2952\n",
            "INFO:tensorflow:loss = 0.024024379, step = 5600 (3.304 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.7018\n",
            "INFO:tensorflow:loss = 0.06395254, step = 5700 (3.254 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.4406\n",
            "INFO:tensorflow:loss = 0.0048274714, step = 5800 (3.288 sec)\n",
            "INFO:tensorflow:global_step/sec: 30.557\n",
            "INFO:tensorflow:loss = 0.009917941, step = 5900 (3.270 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 6000 into /tmp/tmp_uovurve/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.25949794.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-12-28T18:03:49Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmp_uovurve/model.ckpt-6000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [60/600]\n",
            "INFO:tensorflow:Evaluation [120/600]\n",
            "INFO:tensorflow:Evaluation [180/600]\n",
            "INFO:tensorflow:Evaluation [240/600]\n",
            "INFO:tensorflow:Evaluation [300/600]\n",
            "INFO:tensorflow:Evaluation [360/600]\n",
            "INFO:tensorflow:Evaluation [420/600]\n",
            "INFO:tensorflow:Evaluation [480/600]\n",
            "INFO:tensorflow:Evaluation [540/600]\n",
            "INFO:tensorflow:Evaluation [600/600]\n",
            "INFO:tensorflow:Finished evaluation at 2019-12-28-18:03:56\n",
            "INFO:tensorflow:Saving dict for global step 6000: accuracy = 0.9893, global_step = 6000, loss = 0.031558644\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6000: /tmp/tmp_uovurve/model.ckpt-6000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.9893, 'global_step': 6000, 'loss': 0.031558644}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "b8561f20-3405-4f7b-c7e8-320e603f3042",
        "colab_type": "code",
        "id": "sInFXfVDRm0o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "pred_input_fn = lambda: input_fn(x_test, y_test, batch_size=batch_size, num_epochs=1)\n",
        "predict = from_keras.predict(pred_input_fn)\n",
        "\n",
        "predict = [pred[list(pred.keys())[0]] for pred in predict]\n",
        "predict = np.argmax(predict, axis=1)\n",
        "\n",
        "errors = 0\n",
        "for i in range(num_test):\n",
        "  if predict[i] != y_test[i]:\n",
        "    errors += 1\n",
        "print(f\"final accuracy: {(1 - errors / num_test) * 100 :.2f}%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmp_uovurve/model.ckpt-6000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "final accuracy: 98.32%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}